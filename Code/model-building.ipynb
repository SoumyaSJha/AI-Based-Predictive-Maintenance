{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:24.094732Z","iopub.execute_input":"2023-03-28T12:18:24.095385Z","iopub.status.idle":"2023-03-28T12:18:24.163710Z","shell.execute_reply.started":"2023-03-28T12:18:24.095348Z","shell.execute_reply":"2023-03-28T12:18:24.162539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read datasets back in cleanly\ndf_train = pd.read_pickle('/kaggle/input/processedata/traindata5.pkl')\ndf_test = pd.read_pickle('/kaggle/input/processedata/testdata5.pkl')\nsequence_length=5\ndf_train.head(sequence_length)\n#df_train.fillna(0)\n#df_test.fillna(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-28T12:18:24.165811Z","iopub.execute_input":"2023-03-28T12:18:24.166569Z","iopub.status.idle":"2023-03-28T12:18:24.391961Z","shell.execute_reply.started":"2023-03-28T12:18:24.166509Z","shell.execute_reply":"2023-03-28T12:18:24.390610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.dropna(axis='columns', how='all') \n# Next, let's remove a few rows that had invalid data (most likely error reading SMART statistics)\ndf_train = df_train.dropna(axis='rows', how='any')\ndf_test = df_test.dropna(axis='columns', how='all') \ndf_test = df_test.dropna(axis='rows', how='any')\n'''df_train.dropna(how='all', axis=1, inplace=True)\ndf_test.dropna(how='all', axis=1, inplace=True)\ndf_train = df_train.dropna(axis='rows', how='any')\ndf_test = df_test.dropna(axis='rows', how='any')'''\n\n\n#col_list_raw = remove_constant_values(df_failed,\"_raw\")\n#col_list_normalized = remove_constant_values(df_failed,\"_normalized\")\n#col_list = ['date', 'serial_number', 'failure'] \n#col_list = col_list + col_list_raw + col_list_normalized\n\n#df_train = df_train[col_list]\n\n#keep the same columns as the training dataframe \n#df_test = df_test[col_list]\ndf_test = df_test.reindex(columns=df_train.columns)\n\n#fill in the small number of null values in remaining columns \ndf_train = df_train.fillna(0)\ndf_test = df_test.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:24.394715Z","iopub.execute_input":"2023-03-28T12:18:24.395919Z","iopub.status.idle":"2023-03-28T12:18:24.798174Z","shell.execute_reply.started":"2023-03-28T12:18:24.395874Z","shell.execute_reply":"2023-03-28T12:18:24.794815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index data will hold disk serial numbers for relevant samples. We do not need serial numbers as training or testing features\ndrop_columns_list = ['date','serial_number', 'failure', 'sequence_label']\n\nx_train = df_train.drop(columns=drop_columns_list)\nx_test = df_test.drop(columns=drop_columns_list)\n\ny_train = df_train['sequence_label']\ny_test = df_test['sequence_label']","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:24.804142Z","iopub.execute_input":"2023-03-28T12:18:24.806729Z","iopub.status.idle":"2023-03-28T12:18:24.832009Z","shell.execute_reply.started":"2023-03-28T12:18:24.806682Z","shell.execute_reply":"2023-03-28T12:18:24.830669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn import preprocessing\n\n\nx_train = x_train.values.astype(np.float64)\n\n\n\n# X test data\nx_test = x_test.values.astype(np.float64)\n\n\nimport pandas as pd\n\n# Convert numpy arrays to pandas dataframes\nx_train = pd.DataFrame(x_train)\nx_test = pd.DataFrame(x_test)\n\n# Filter out extra columns in the training data\ncols_to_drop = set(x_train.columns) - set(x_test.columns)\nx_train = x_train.drop(cols_to_drop, axis=1)\n\n# Add missing columns in the testing data\nmissing_cols = set(x_train.columns) - set(x_test.columns)\nfor col in missing_cols:\n    x_test[col] = 0\n\n# Apply scaler\nscaler = preprocessing.StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:24.834109Z","iopub.execute_input":"2023-03-28T12:18:24.834893Z","iopub.status.idle":"2023-03-28T12:18:26.452350Z","shell.execute_reply.started":"2023-03-28T12:18:24.834850Z","shell.execute_reply":"2023-03-28T12:18:26.450933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:26.454378Z","iopub.execute_input":"2023-03-28T12:18:26.455321Z","iopub.status.idle":"2023-03-28T12:18:26.478479Z","shell.execute_reply.started":"2023-03-28T12:18:26.455207Z","shell.execute_reply":"2023-03-28T12:18:26.475762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Reshaping training and test data for LSTM\")\nprint(\"- Original Train Shape:\", x_train.shape)\n\nx_train = x_train.reshape(int(x_train.shape[0]/sequence_length), sequence_length, x_train.shape[1])\nx_test = x_test.reshape(int(x_test.shape[0]/sequence_length), sequence_length, x_test.shape[1])\n\nprint(\"- Final Train Shape:   \", x_train.shape)\ndata_shape = x_train.shape[1:]","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:26.482612Z","iopub.execute_input":"2023-03-28T12:18:26.485864Z","iopub.status.idle":"2023-03-28T12:18:26.506787Z","shell.execute_reply.started":"2023-03-28T12:18:26.485828Z","shell.execute_reply":"2023-03-28T12:18:26.505404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Y train data\ny_train = y_train.values[0::sequence_length]\n\n# Y test data\ny_test = y_test.values[0::sequence_length]\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:26.525574Z","iopub.execute_input":"2023-03-28T12:18:26.530263Z","iopub.status.idle":"2023-03-28T12:18:26.781983Z","shell.execute_reply.started":"2023-03-28T12:18:26.530214Z","shell.execute_reply":"2023-03-28T12:18:26.780680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\n\ndef matthews_correlation(y_true, y_pred):\n    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1-y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true * (1-y_pred), 0, 1)))\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator / (denominator + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:26.787032Z","iopub.execute_input":"2023-03-28T12:18:26.790040Z","iopub.status.idle":"2023-03-28T12:18:41.680210Z","shell.execute_reply.started":"2023-03-28T12:18:26.789996Z","shell.execute_reply":"2023-03-28T12:18:41.679066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_features = x_train.shape[2]\ntimestamp = 5\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Activation\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras.optimizers import Adagrad\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom keras import metrics\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\ndp_lvl = 0.2\nregularizer_lvl = 0.002\nunits = 64\n\nepochs_num = 100\nlearning_rate = 0.001\ndecay_rate = 3 * learning_rate / epochs_num\noptimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None,decay = decay_rate)\n#optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weights_dict = dict(enumerate(class_weights))\n\nmodel = Sequential()\nmodel.add(LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]),dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\nmodel.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\nmodel.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  False ))\nmodel.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n\n    # Dropout is a technique used to tackle Overfitting.\nmodel.add(Dropout (0.2))\nmodel.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\nmodel.add(Dense(1, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n#model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy',metrics.Recall(),matthews_correlation])\nmodel.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=[metrics.Recall(),'accuracy'])\nmodel_fn = \"best_model.h5\"\nmodel_filepath=model_fn\n\ncheckpoint = ModelCheckpoint(model_filepath, monitor='val_recall', verbose=1, save_best_only=True, mode='max')\nes = EarlyStopping(patience=20, verbose=1)\n\ncallbacks_list = [es, checkpoint]\n# Compute the class weights\n\n\n# Train the model on the entire training set with class weights and early stopping\n#history = model.fit(x_train, y_train, epochs=100, batch_size=16,  callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n                       #])\nhistory = model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1,  shuffle=True,class_weight=class_weights_dict, callbacks=callbacks_list)\n# Print the final training accuracy, validation accuracy, and recall\n#print(\"Final training accuracy:\", history.history['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T12:18:41.682174Z","iopub.execute_input":"2023-03-28T12:18:41.683072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\n\n# convert probabilities to binary predictions (0 or 1)\ny_pred_binary = (y_pred > 0.5).astype(int)\n\n# compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_binary)\nprint(cm)\nTP = cm[0][0]\nTN = cm[1][1]\nFP = cm[0][1]\nFN = cm[1][0]\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\nmodel = load_model('best_model.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dp_lvl = 0.2\nregularizer_lvl = 0.001\nunits = 64\n\nepochs_num = 50\nlearning_rate = 0.001\ndecay_rate = 3 * learning_rate / epochs_num\noptimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate)\n\nnew_layers = [\n    LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]), dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=False),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dropout(0.2),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dense(1, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl))\n]\n\n# Replace the existing layers with the new ones\nfor i, layer in enumerate(new_layers):\n    model.layers[i] = layer\n\n# Compile the modified model\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[metrics.Recall(), 'accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1,  shuffle=True, class_weight=class_weights_dict, callbacks=callbacks_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load weights from best model\n#model.load_weights(model_filepath)\n\n# evaluate model against test set\n#model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', metrics.Recall()]) \nscores = model.evaluate(x_test, y_test, verbose=1)\nprint(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dp_lvl = 0.2\nregularizer_lvl = 0.002\nunits = 64\n\nepochs_num = 50\nlearning_rate = 0.0001\ndecay_rate = 3 * learning_rate / epochs_num\noptimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate)\n\nnew_layers = [\n    LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]), dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=False),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dropout(0.2),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dense(1, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl))\n]\n\n# Replace the existing layers with the new ones\nfor i, layer in enumerate(new_layers):\n    model.layers[i] = layer\n\n# Compile the modified model\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[ 'accuracy',metrics.Recall()])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1,  shuffle=True, class_weight=class_weights_dict, callbacks=callbacks_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\n\n# convert probabilities to binary predictions (0 or 1)\ny_pred_binary = (y_pred > 0.5).astype(int)\n\n# compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_binary)\nprint(cm)\nTP = cm[0][0]\nTN = cm[1][1]\nFP = cm[0][1]\nFN = cm[1][0]\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate(x_test, y_test, verbose=1)\nprint(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dp_lvl = 0.2\nregularizer_lvl = 0.001\nunits = 64\n\nepochs_num = 50\nlearning_rate = 0.0001\ndecay_rate = 3 * learning_rate / epochs_num\noptimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate)\n\nnew_layers = [\n    LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]), dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=True),\n    LSTM(units, dropout=dp_lvl, recurrent_dropout=dp_lvl, return_sequences=False),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dropout(0.2),\n    Dense(units, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl)),\n    Dense(1, activation='tanh', activity_regularizer=regularizers.l2(regularizer_lvl))\n]\n\n# Replace the existing layers with the new ones\nfor i, layer in enumerate(new_layers):\n    model.layers[i] = layer\n\n# Compile the modified model\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[metrics.Recall(),'accuracy'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1,  shuffle=True, class_weight=class_weights_dict, callbacks=callbacks_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\n\n# convert probabilities to binary predictions (0 or 1)\ny_pred_binary = (y_pred > 0.5).astype(int)\n\n# compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_binary)\nprint(cm)\nTP = cm[0][0]\nTN = cm[1][1]\nFP = cm[0][1]\nFN = cm[1][0]\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprint(\"Accuracy:\", accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate(x_test, y_test, verbose=1)\nprint(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('hi.h5')","metadata":{},"execution_count":null,"outputs":[]}]}